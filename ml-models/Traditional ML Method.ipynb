{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchtext\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import HeadlineDataset, collate_fn, generate_vocab_map\n",
    "from src.models import LSTMClassificationModel\n",
    "from src.text_cleaning import clean_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18724\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "data = pd.read_csv(\"data/data_all.csv\")\n",
    "data[\"tokenized\"] = data[\"text\"].apply(clean_text).apply(tokenizer)\n",
    "data.head()\n",
    "print(len(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5862\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "location_less = data['location'].isnull().sum()\n",
    "print(location_less)\n",
    "text_less = 0\n",
    "for i in range(len(data)):\n",
    "    text = data['tokenized'][i]\n",
    "    if (len(text) < 3):\n",
    "        text_less += 1\n",
    "    if data['location'] is None:\n",
    "        location_less += 1\n",
    "print(text_less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.08128604998932 ModeResult(mode=array([20]), count=array([1394])) 17.0 5.737168059418903\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "token_length = []\n",
    "for i in range(len(data)):\n",
    "    text = data['tokenized'][i]\n",
    "    token_length.append(len(text))\n",
    "token_length = np.array(token_length)\n",
    "mean_length = np.mean(token_length)\n",
    "mode_length = stats.mode(token_length)\n",
    "median_length = np.median(token_length)\n",
    "variance = np.sqrt(np.var(token_length))\n",
    "print(mean_length,mode_length, median_length, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    keyword  location                                               text  \\\n",
      "0   0   attacked     India  Largscale violence in #Bhainsa, dist Nirmal of...   \n",
      "1   1    trouble       NaN  Hi there. I'm sorry to hear that you are havin...   \n",
      "2   2     derail       NaN  Design of AR-15 could derail charges tied to p...   \n",
      "3   3  emergency  The Void  I've never posed in the office block coz whene...   \n",
      "4   4   hostages       NaN  “We are not citizens. We never were. We are ca...   \n",
      "\n",
      "   label                                          tokenized  \n",
      "0      0  [largscale, violence, in, bhainsa, dist, nirma...  \n",
      "1      1  [hi, there, im, sorry, to, hear, that, you, ar...  \n",
      "2      0  [design, of, arnumber, could, derail, charges,...  \n",
      "3      0  [ive, never, posed, in, the, office, block, co...  \n",
      "4      0  [we, are, not, citizens, we, never, were, we, ...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "train_val_data, test_data = train_test_split(data, train_size=0.9, test_size=0.1, shuffle=False)\n",
    "print(train_val_data.head())\n",
    "print(type(train_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/songmingliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    keyword  location                                               text  \\\n",
      "0   0   attacked     India  Largscale violence in #Bhainsa, dist Nirmal of...   \n",
      "1   1    trouble       NaN  Hi there. I'm sorry to hear that you are havin...   \n",
      "2   2     derail       NaN  Design of AR-15 could derail charges tied to p...   \n",
      "3   3  emergency  The Void  I've never posed in the office block coz whene...   \n",
      "4   4   hostages       NaN  “We are not citizens. We never were. We are ca...   \n",
      "\n",
      "   label                                          tokenized  \\\n",
      "0      0  [largscale, violence, in, bhainsa, dist, nirma...   \n",
      "1      1  [hi, there, im, sorry, to, hear, that, you, ar...   \n",
      "2      0  [design, of, arnumber, could, derail, charges,...   \n",
      "3      0  [ive, never, posed, in, the, office, block, co...   \n",
      "4      0  [we, are, not, citizens, we, never, were, we, ...   \n",
      "\n",
      "                               tokenized_after_clean  \n",
      "0  [largscal, violenc, in, bhainsa, dist, nirmal,...  \n",
      "1  [hi, there, im, sorri, to, hear, that, you, ar...  \n",
      "2  [design, of, arnumb, could, derail, charg, tie...  \n",
      "3  [ive, never, pose, in, the, offic, block, coz,...  \n",
      "4  [we, are, not, citizen, we, never, were, we, a...  \n"
     ]
    }
   ],
   "source": [
    "# !pip3 install nltk\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def stemming(txt):\n",
    "    words=[ps.stem(word) for word in txt]\n",
    "    return words\n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokenize):\n",
    "    clean=[word for word in tokenize if word not in stopwords]\n",
    "    return clean\n",
    "\n",
    "train_val_data['tokenized_after_clean']=train_val_data['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "test_data['tokenized_after_clean']=test_data['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "train_val_data['tokenized_after_clean']=train_val_data['tokenized'].apply(lambda x: stemming(x))\n",
    "test_data['tokenized_after_clean']=test_data['tokenized'].apply(lambda x: stemming(x))\n",
    "\n",
    "\n",
    "print(train_val_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = (2,3)\n",
    "max_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(train_val_data['label'])\n",
    "Test_Y = Encoder.fit_transform(test_data['label'])\n",
    "\n",
    "Train_X = train_val_data['tokenized_after_clean']\n",
    "Test_X = test_data['tokenized_after_clean']\n",
    "\n",
    "# # ---------TfidfVectorizer---------\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=5000, preprocessor=' '.join, stop_words='english')\n",
    "# Tfidf_vect.fit(Train_X)\n",
    "# Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "# Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "# ---------BoW---------\n",
    "Tfidf_vect = CountVectorizer(preprocessor=''.join, stop_words='english', ngram_range=ngram_range, max_features=max_features)\n",
    "Tfidf_vect.fit(Train_X)\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "\n",
    "# -----------SMOTE-------------\n",
    "# !pip install -U imbalanced-learn\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "# sm = SMOTE(random_state=42)\n",
    "# Train_X_Tfidf, Train_Y = sm.fit_resample(Train_X_Tfidf, Train_Y)\n",
    "# Test_X_Tfidf, Test_Y = sm.fit_resample(Test_X_Tfidf, Test_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  50.0\n",
      "Naive Bayes f-1 Score ->  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1319, 1319],\n",
       "       [   0,    0]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(\"Naive Bayes f-1 Score -> \",f1_score(predictions_NB, Test_Y))\n",
    "\n",
    "confusion_matrix(predictions_NB, Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  50.0\n",
      "SVM f-1 Score ->  0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0],\n",
       "       [1319, 1319]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "SVM = svm.SVC(C=10.5, kernel='linear', gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM f-1 Score -> \",f1_score(predictions_SVM, Test_Y))\n",
    "confusion_matrix(predictions_SVM, Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  84.6095526914329\n",
      "SVM f-1 Score ->  0.830550918196995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1237,  324],\n",
       "       [  82,  995]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.SVC(kernel='rbf', C=7.5)\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM f-1 Score -> \",f1_score(predictions_SVM, Test_Y))\n",
    "confusion_matrix(predictions_SVM, Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  65.5420773313116\n",
      "SVM f-1 Score ->  0.5172596919808815\n",
      "564\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=7.5, kernel='sigmoid', gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM f-1 Score -> \",f1_score(predictions_SVM, Test_Y))\n",
    "print(np.sum(np.array(predictions_SVM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  54.51099317664898\n",
      "SVM f-1 Score ->  0.16550764951321278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1319, 1200],\n",
       "       [   0,  119]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.SVC(C=9.0, kernel='poly', gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM f-1 Score -> \",f1_score(predictions_SVM, Test_Y))\n",
    "confusion_matrix(predictions_SVM, Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  80.6292645943897\n",
      "SVM f-1 Score ->  0.8026264967168791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1319, 1200],\n",
       "       [   0,  119]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log = LogisticRegression(random_state=0, C=1.1)\n",
    "log.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_log = log.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_log, Test_Y)*100)\n",
    "print(\"SVM f-1 Score -> \",f1_score(predictions_log, Test_Y))\n",
    "confusion_matrix(predictions_SVM, Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = naive_bayes.MultinomialNB()\n",
    "# base_estimator = svm.SVC(kernel='rbf', C=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging  Accuracy Score ->  80.17437452615617\n",
      "Bagging regression f-1 Score ->  0.7968932038834952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc = BaggingClassifier(base_estimator=base_estimator, n_estimators=800)\n",
    "bc.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "predictions_bc = bc.predict(Test_X_Tfidf)\n",
    "print(\"Bagging  Accuracy Score -> \",accuracy_score(predictions_bc, Test_Y)*100)\n",
    "print(\"Bagging regression f-1 Score -> \",f1_score(predictions_bc, Test_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging  Accuracy Score ->  82.48673237300986\n",
      "Bagging regression f-1 Score ->  0.8206521739130436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=None, n_estimators=800)\n",
    "abc.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "predictions_abc = abc.predict(Test_X_Tfidf)\n",
    "print(\"Bagging  Accuracy Score -> \",accuracy_score(predictions_abc, Test_Y)*100)\n",
    "print(\"Bagging regression f-1 Score -> \",f1_score(predictions_abc, Test_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging  Accuracy Score ->  84.64746019711903\n",
      "Bagging regression f-1 Score ->  0.8378053664397277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=800)\n",
    "gbc.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "predictions_gbc = gbc.predict(Test_X_Tfidf)\n",
    "print(\"Bagging  Accuracy Score -> \",accuracy_score(predictions_gbc, Test_Y)*100)\n",
    "print(\"Bagging regression f-1 Score -> \",f1_score(predictions_gbc, Test_Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging  Accuracy Score ->  83.51023502653526\n",
      "Bagging regression f-1 Score ->  0.8262085497403117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=550, criterion='gini')\n",
    "rf.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "predictions_rf = rf.predict(Test_X_Tfidf)\n",
    "print(\"Bagging  Accuracy Score -> \",accuracy_score(predictions_rf, Test_Y)*100)\n",
    "print(\"Bagging regression f-1 Score -> \",f1_score(predictions_rf, Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install lightgbm\n",
    "import lightgbm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "fit = lightgbm.Dataset(Train_X_Tfidf, Train_Y)\n",
    "val = lightgbm.Dataset(Test_X_Tfidf, Test_Y, reference=fit)\n",
    "\n",
    "model = lightgbm.train(\n",
    "    params={\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'binary'\n",
    "    },\n",
    "    train_set=fit,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {metrics.roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {metrics.log_loss(y_test, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
